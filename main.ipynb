{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110307,"databundleVersionId":13386175,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T22:02:24.828483Z","iopub.execute_input":"2025-08-28T22:02:24.828684Z","iopub.status.idle":"2025-08-28T22:02:26.858950Z","shell.execute_reply.started":"2025-08-28T22:02:24.828666Z","shell.execute_reply":"2025-08-28T22:02:26.857956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install pandas numpy scikit-learn unidecode fasttext==0.9.2\n!apt-get update -qq\n!apt-get install -y -qq autoconf automake libtool pkg-config curl build-essential\n\n!git clone --depth 1 https://github.com/openvenues/libpostal\n!cd libpostal && ./bootstrap.sh\n!cd libpostal && ./configure --datadir=/content/libpostal_data\n!cd libpostal && make -j\"$(nproc)\"\n!cd libpostal && make install\n\n!ldconfig\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U pip setuptools wheel\n!pip install postal\n!pip install -q sentence-transformers faiss-cpu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, re, os, gc\nfrom unidecode import unidecode\nimport fasttext\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import normalize as sk_normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n_ABBR = {\n    # mahalle\n    r\"\\bmah\\.?\\b\": \"mahalle\", r\"\\bmh\\.?\\b\": \"mahalle\", r\"\\bmhl\\b\": \"mahalle\",\n    r\"\\bmahallesi\\b\": \"mahalle\",\n    # sokak\n    r\"\\bsok\\.?\\b\": \"sokak\", r\"\\bsk\\.?\\b\": \"sokak\", r\"\\bsoka?\\.?\\b\": \"sokak\",\n    # cadde\n    r\"\\bcadd?\\.?\\b\": \"cadde\", r\"\\bcad\\.?\\b\": \"cadde\", r\"\\bcd\\.?\\b\": \"cadde\",\n    # bulvar\n    r\"\\bblv\\.?\\b\": \"bulvar\", r\"\\bbulv?\\.?\\b\": \"bulvar\",\n    # apartman / site / blok\n    r\"\\bapt\\.?\\b\": \"apartman\", r\"\\bap\\.?\\b\": \"apartman\",\n    r\"\\bsitesi?\\b\": \"sitesi\", r\"\\bsit\\.\\b\": \"sitesi\",\n    r\"\\bblok\\b\": \"blok\",\n    # organize sanayi\n    r\"\\bosb\\b\": \"organize sanayi bolgesi\", r\"\\borg\\.?\\b\": \"organize\",\n    # posta kutusu\n    r\"\\bpk\\.?\\b\": \"postakutusu\",\n    # merkez\n    r\"\\bmerkez\\b\": \"merkez\"\n}\n_punct_re = re.compile(r\"[^a-z0-9ğüşöçıİĞÜŞÖÇ]+\", flags=re.IGNORECASE)\n\ndef _casefold_tr(s: str) -> str:\n    # I/İ\n    return (s or \"\").strip().casefold()\n\ndef normalize_text(s: str) -> str:\n    if not isinstance(s, str):\n        return \"\"\n    s = _casefold_tr(s)\n\n    # kısaltmalar\n    for pat, repl in _ABBR.items():\n        s = re.sub(pat, repl, s)\n\n    s = re.sub(r\"\\b(?:no|numara)\\s*[:\\-]?\\s*([0-9]+(?:/[0-9a-z])?)\\b\", r\"no \\1\", s)\n    s = re.sub(r\"\\bno\\s+(\\d+)[\\s/]*([a-z0-9])\\b\", r\"no \\1 d \\2\", s)\n    s = re.sub(r\"\\bkat\\s*([0-9]+)\\b\", r\"kat \\1\", s)\n    s = re.sub(r\"\\bd\\.?\\s*([0-9]+)\\b\", r\"d \\1\", s)        \n    s = re.sub(r\"\\bdr\\.?\\s+(?=[a-zğüşöçı])\", \"doktor \", s)\n    s = re.sub(r\"\\bdaire\\s*([0-9]+)\\b\", r\"d \\1\", s)\n    s = re.sub(r\"\\bblok\\s*([a-z0-9]+)\\b\", r\"blok \\1\", s)\n    s = re.sub(r\"([a-zğüşöçı])(\\d)\", r\"\\1 \\2\", s)\n    s = re.sub(r\"(\\d)([a-zğüşöçı])\", r\"\\1 \\2\", s)\n    s = _punct_re.sub(\" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef normalize_text_ascii(s: str) -> str:\n    return normalize_text(unidecode(s or \"\"))\n\ndef fingerprint(s: str) -> str:\n    s = normalize_text(s)\n    toks = s.split()\n    keep = {\"no\",\"kat\",\"d\",\"blok\"}\n    toks = [t for t in toks if (len(t) > 1 or t in keep)]\n    toks = sorted(set(toks))\n    return \" \".join(toks)\n\ndef build_text(df: pd.DataFrame) -> pd.Series:\n    return (df[\"norm\"] + \" || \" + df[\"norm_ascii\"]).astype(str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"data/train.csv\")\ntest  = pd.read_csv(\"data/test.csv\")\n\nif \"id\" not in test.columns:\n    test = test.reset_index().rename(columns={\"index\": \"id\"})\n\ntrain[\"label\"] = train[\"label\"].astype(str)\ntrain[\"address\"] = train[\"address\"].astype(str)\ntest[\"address\"]  = test[\"address\"].astype(str)\n\nprint(\"train:\", train.shape, \"| test:\", test.shape, \"| sınıf sayısı:\", train[\"label\"].nunique())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef majority_map(df: pd.DataFrame, key_col: str, label_col: str,\n                 min_count: int = 1, purity: float = 0.0) -> pd.Series:\n    vc = df.groupby(key_col)[label_col].value_counts()\n    vc = vc.to_frame(\"cnt\").reset_index()\n    total = vc.groupby(key_col)[\"cnt\"].transform(\"sum\")\n    vc[\"frac\"] = vc[\"cnt\"] / total\n    vc = vc.sort_values([\"cnt\"], ascending=False).drop_duplicates([key_col])\n    ok = (vc[\"cnt\"] >= min_count) & (vc[\"frac\"] >= purity)\n    return pd.Series(vc.loc[ok, label_col].values,\n                     index=vc.loc[ok, key_col].values)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# normalize / ascii / fingerprint \nfor col, fn in [(\"norm\", normalize_text), (\"norm_ascii\", normalize_text_ascii), (\"fp\", fingerprint)]:\n    train[col] = train[\"address\"].map(fn)\n    test[col]  = test[\"address\"].map(fn)\n\nnorm2label       = majority_map(train, \"norm\", \"label\", min_count=3, purity=0.75)\nnorm_ascii2label = majority_map(train, \"norm_ascii\", \"label\", min_count=3, purity=0.75)\nfp2label         = majority_map(train, \"fp\", \"label\", min_count=3, purity=0.75)\n\nglobal_top_label = train[\"label\"].value_counts().idxmax()\n\npred_rule = pd.Series([None]*len(test), index=test.index, dtype=\"object\")\n\nm_norm  = test[\"norm\"].isin(norm2label.index)\npred_rule[m_norm] = test.loc[m_norm, \"norm\"].map(norm2label)\n\nm_ascii = pred_rule.isna() & test[\"norm_ascii\"].isin(norm_ascii2label.index)\npred_rule[m_ascii] = test.loc[m_ascii, \"norm_ascii\"].map(norm_ascii2label)\n\nm_fp    = pred_rule.isna() & test[\"fp\"].isin(fp2label.index)\npred_rule[m_fp] = test.loc[m_fp, \"fp\"].map(fp2label)\n\ncoverage = (~pred_rule.isna()).mean()*100\nprint(f\"[RULE] kapsama: %{coverage:.2f}  (norm: {m_norm.mean()*100:.2f} / ascii: {m_ascii.mean()*100:.2f} / fp: {m_fp.mean()*100:.2f})\")\n\nneed_ml_idx = pred_rule[pred_rule.isna()].index\nprint(\"ML ile sınıflandırılacak test sayısı:\", len(need_ml_idx))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== LIBPOSTAL PARSER KATMANI =====\nfrom postal.parser import parse_address\nimport re\n\ndef lp_parse_row(s: str):\n    try:\n        parts = dict(parse_address(s or \"\"))\n    except Exception:\n        parts = {}\n    house = parts.get(\"house_number\") or parts.get(\"house\") or \"\"\n    road = parts.get(\"road\") or parts.get(\"pedestrian\") or parts.get(\"footway\") or \"\"\n    # birim/kat/daire\n    unit = (parts.get(\"unit\") or parts.get(\"level\") or parts.get(\"staircase\") \n            or parts.get(\"entrance\") or parts.get(\"po_box\") or \"\")\n    # mahalle/ilçe etiket karması\n    neighbourhood = (parts.get(\"neighbourhood\") or parts.get(\"suburb\") \n                     or parts.get(\"city_district\") or parts.get(\"state_district\") or \"\")\n    city = parts.get(\"city\") or parts.get(\"state\") or \"\"   # TR’de bazen 'state' = İl\n    postcode = parts.get(\"postcode\") or \"\"\n    return house, road, unit, neighbourhood, city, postcode\n\n\ndef lp_clean_road(road: str) -> str:\n    r = normalize_text(road)\n    return r\n\ndef make_lp_key(addr: str) -> str:\n    h, road, unit, suburb, city, pc = lp_parse_row(addr)\n    road = lp_clean_road(road)\n    # no + road anahtar +varsa posta kodu\n    key = f\"{h} {road}\".strip()\n    if pc: key = f\"{key} #{pc}\"\n    return re.sub(r\"\\s+\", \" \", key).strip()\n\ntrain[\"lp_key\"] = train[\"address\"].map(make_lp_key)\ntest[\"lp_key\"]  = test[\"address\"].map(make_lp_key)\n\n\nlp2label = majority_map(train, \"lp_key\", \"label\", min_count=3, purity=0.80)\n\n# kural tabanına “lp_key” kuralı\nm_lp = pred_rule.isna() & test[\"lp_key\"].isin(lp2label.index)\npred_rule[m_lp] = test.loc[m_lp, \"lp_key\"].map(lp2label)\n\ncoverage = (~pred_rule.isna()).mean()*100\nprint(f\"[RULE+LP] kapsama: %{coverage:.2f}  (+lp: {m_lp.mean()*100:.2f})\")\n\nneed_ml_idx = pred_rule[pred_rule.isna()].index\nprint(\"ML ile sınıflandırılacak test sayısı (LP sonrası):\", len(need_ml_idx))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, psutil\nprint(\"vCPU:\", os.cpu_count(), \"| logical:\", psutil.cpu_count(logical=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ft_train_path = \"ft_train.txt\"\nwith open(ft_train_path, \"w\", encoding=\"utf-8\") as f:\n    for lbl, txt in zip(train[\"label\"].tolist(), build_text(train).tolist()):\n        f.write(f\"__label__{lbl} {txt}\\n\")\n\nmodel = fasttext.train_supervised(\n    input=ft_train_path,\n    lr=0.7,\n    epoch=35,\n    wordNgrams=4,\n    dim=300,\n    loss=\"softmax\",\n    minn=2, maxn=5\n)\nprint(\"fastText modeli eğitildi.\")\n\ntexts_need = build_text(test.loc[need_ml_idx]).tolist()\nft_labels, ft_probs = model.predict(texts_need, k=5)  # top-5 alıyoruz (re-rank için)\n\ntop1 = [labs[0].replace(\"__label__\", \"\") if len(labs)>0 else global_top_label for labs in ft_labels]\npred_ml_top1 = pd.Series(top1, index=need_ml_idx, dtype=\"object\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== SBERT + FAISS RERANKER (TF-IDF yerine) =====\nimport torch, numpy as np, pandas as pd, faiss, gc\nfrom sentence_transformers import SentenceTransformer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nemb = SentenceTransformer(MODEL_NAME, device=device)\n\ndef text_for_embed(df: pd.DataFrame) -> pd.Series:\n    return build_text(df).astype(str).fillna(\"\")\n\n# ---- zayıf örnek eşiği ve adaylar ----\nP1_THR, M_THR = 0.35, 0.03\nTOPK_FT = 5\nALPHA = 0.5         # blend: 0.5*cos + 0.5*ft_prob\nMIN_LABEL_COUNT = 5 # centroid için minimum örnek\n\nft_p1 = np.array([ (p[0] if len(p)>0 else 0.0) for p in ft_probs ], dtype=np.float32)\nft_p2 = np.array([ (p[1] if len(p)>1 else 0.0) for p in ft_probs ], dtype=np.float32)\nweak_mask = (ft_p1 < P1_THR) | ((ft_p1 - ft_p2) < M_THR)\nweak_idx = np.where(weak_mask)[0]\nprint(f\"[SBERT] zayıf örnek: {len(weak_idx)} / {len(need_ml_idx)}\")\n\n# fastText top-K adaylar ve olasılıkları\nft_topk = []\nft_topk_prob = []\nfor labs, probs in zip(ft_labels, ft_probs):\n    labs = [l.replace(\"__label__\",\"\") for l in labs[:TOPK_FT]]\n    prs  = [float(p) for p in probs[:TOPK_FT]]\n    ft_topk.append(labs)\n    ft_topk_prob.append(prs)\n\n# ---- sadece gereken label'ların centroid embedding'i ----\nneeded_labels = sorted({l for i in weak_idx for l in ft_topk[i]})\nprint(f\"[SBERT] centroid hesaplanacak label sayısı: {len(needed_labels)}\")\n\n# chunk'lı embedding + centroid biriktirme\nlabel_to_sum = {l: None for l in needed_labels}\nlabel_to_cnt = {l: 0    for l in needed_labels}\n\nBATCH = 2048  # embedding için chunk\ntrain_text = text_for_embed(train)\ny_lab = train[\"label\"].astype(str).values\n\nfor s in range(0, len(train), BATCH):\n    t = train_text.iloc[s:s+BATCH].tolist()\n    X = emb.encode(t, convert_to_numpy=True, normalize_embeddings=True, batch_size=128, show_progress_bar=False)\n    yl = y_lab[s:s+BATCH]\n    # yalnız needed_labels için topla\n    for l in np.unique(yl):\n        if l not in label_to_sum:\n            continue\n        idx = np.where(yl == l)[0]\n        if len(idx)==0:\n            continue\n        vec = X[idx].sum(axis=0, dtype=np.float32)  # sum\n        if label_to_sum[l] is None:\n            label_to_sum[l] = vec\n        else:\n            label_to_sum[l] += vec\n        label_to_cnt[l] += len(idx)\n    del X\n    gc.collect()\n\n# centroid matrisini hazırla (L x D)\nlabels_ok = []\ncentroids = []\nfor l in needed_labels:\n    cnt = label_to_cnt[l]\n    if cnt >= 1:\n        v = label_to_sum[l] / max(1, cnt)\n        # zaten normalize encode kullandık ama tedbir alfdık:\n        v = v / (np.linalg.norm(v) + 1e-9)\n    else:\n        # hiç yoksa sıfır vektör\n        v = np.zeros(emb.get_sentence_embedding_dimension(), dtype=np.float32)\n    labels_ok.append(l)\n    centroids.append(v.astype(np.float32))\ncentroids = np.vstack(centroids).astype(np.float32)\nprint(\"[SBERT] centroid matrix:\", centroids.shape)\n\n# ---- FAISS index (cosine ~ inner product on normalized vectors) ----\nindex = faiss.IndexFlatIP(centroids.shape[1])\nindex.add(centroids)  # L adet centroid\n\n# test (need_ml) tarafı – zayıflar için embedding\nneed_text = text_for_embed(test.loc[need_ml_idx]).tolist()\n\nfinal_ml = pred_ml_top1.loc[need_ml_idx].astype(str).to_numpy()\nchanged = 0\n\n# encode zayıflar\nweak_texts = [need_text[i] for i in weak_idx]\nZ = emb.encode(weak_texts, convert_to_numpy=True, normalize_embeddings=True, batch_size=128, show_progress_bar=False)\n\n# FAISS ile topK centroid çek (geniş tutup sonra aday kesişimi alacağız)\nK_F = min(50, len(labels_ok))\nD, I = index.search(Z, K_F)  # (Nweak x K_F)\n\n# label id -> row index map\nlab2row = {l:i for i,l in enumerate(labels_ok)}\n\nfor pos, j in enumerate(weak_idx):\n    # FAISS adayları\n    faiss_rows = I[pos]\n    faiss_labs = [labels_ok[r] for r in faiss_rows if r != -1]\n    faiss_scores = D[pos]  # cosine (IP)\n\n    # fastText adayları\n    cand_labs = ft_topk[j]\n    cand_probs = ft_topk_prob[j]\n\n    # kesişim: FAISS adayları ∩ fastText adayları\n    inter = [(lbl, faiss_scores[faiss_labs.index(lbl)] if lbl in faiss_labs else 0.0,\n              cand_probs[cand_labs.index(lbl)] if lbl in cand_labs else 0.0)\n             for lbl in set(cand_labs)]\n\n    # hiçbir kesişim yoksa fastText top1'i koru\n    if not inter:\n        continue\n\n    # blend skor\n    best_lbl, best_score = final_ml[j], -1e9\n    for lbl, cos, p_ft in inter:\n        cnt = label_to_cnt.get(lbl, 0)\n        if cnt < MIN_LABEL_COUNT:\n            cos *= 0.3  # az örneğe ceza\n        score = ALPHA * float(cos) + (1-ALPHA) * float(p_ft)\n        if score > best_score:\n            best_lbl, best_score = lbl, score\n\n    # sadece anlamlı iyileşmede değiştir\n    if best_lbl != final_ml[j] and best_score > (ALPHA*0 + (1-ALPHA)*ft_p1[j] + 1e-6):\n        final_ml[j] = best_lbl\n        changed += 1\n\nprint(f\"[SBERT] değişen zayıf örnek: {changed}/{len(weak_idx)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# SUBMISSION BUILDER (fastText/FAISS/SBERT → submission.csv)\n# =========================\nimport pandas as pd\nimport numpy as np\nimport time, os, re\n\nassert \"test\" in globals(), \"test DataFrame yok.\"\nif \"id\" not in test.columns:\n    test = test.reset_index().rename(columns={\"index\": \"id\"})\nassert \"pred_rule\" in globals(), \"pred_rule yok (kural tabanı oluşturulmalı).\"\nassert \"need_ml_idx\" in globals(), \"need_ml_idx yok (kural sonrası ML'e gidecek indeksler).\"\n\ndef ensure_global_top_label():\n    if \"global_top_label\" in globals():\n        return str(global_top_label)\n    elif \"train\" in globals() and \"label\" in train.columns:\n        return str(train[\"label\"].astype(str).value_counts().idxmax())\n    else:\n        return \"0\"\n\nglobal_top_label = ensure_global_top_label()\n\n# --- fastText tahmini  ---\nif \"pred_ml_top1\" in globals():\n    ft_series = pred_ml_top1.astype(str)\nelse:\n    assert \"ft_labels\" in globals(), \"Ne 'pred_ml_top1' ne de 'ft_labels' var.\"\n    top1 = [(labs[0].replace(\"__label__\",\"\") if len(labs)>0 else global_top_label) for labs in ft_labels]\n    if \"need_ml_idx\" in globals() and len(top1) == len(need_ml_idx):\n        tgt_idx = need_ml_idx\n    elif len(top1) == len(test):\n        tgt_idx = test.index\n    else:\n        raise ValueError(f\"fastText tahmin sayısı ({len(top1)}) ile hedef indeks uyuşmuyor.\")\n    ft_series = pd.Series(top1, index=tgt_idx, dtype=\"object\")\n\n# --- final_pred: kural + ML birleşimi ---\nfinal_pred = pred_rule.astype(\"object\").copy()\n\n# eğer SBERT/FAISS rerankerdan gelen final_ml varsa onu yaz yoksa fastText top-1\nif \"final_ml\" in globals():\n    final_pred.loc[need_ml_idx] = pd.Series(final_ml, index=need_ml_idx).astype(str).values\nelse:\n    final_pred.loc[ft_series.index] = ft_series.values\n\n# Boş kalanlar global_top_label\nfinal_pred.fillna(str(global_top_label), inplace=True)\nfinal_pred = final_pred.astype(str)\n\n# --- submission ---\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"label\": final_pred})\n# label tamamen sayısalsa int'e çevir\nif submission[\"label\"].str.fullmatch(r\"\\d+\").all():\n    submission[\"label\"] = submission[\"label\"].astype(int)\n\nsubmission = submission.sort_values(\"id\").reset_index(drop=True)\nout_path = \"submission.csv\"\nsubmission.to_csv(out_path, index=False)\nuniq = submission[\"label\"].nunique()\nvc = submission[\"label\"].value_counts(normalize=True)\ntop_lab = vc.index[0]\ntop_share = vc.iloc[0] * 100\n\nprint(f\"✅ submission kaydedildi → {out_path}\")\nprint(f\"satır: {len(submission)} | tekil label: {uniq} | en sık label: {top_lab} (%{top_share:.2f})\")\nprint(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tekrar iyileştirme için ekli kısımlar ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}